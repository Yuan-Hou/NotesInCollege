# 基础知识

计算学习理论是机器学习的理论基础，目的是分析学习任务的困难本质，为学习算法提供理论保证，并根据分析结果指导算法设计。

## 概念回顾

### 机器学习基本术语回顾
1. 样本：关于一个对象各种属性的描述 eg. 西瓜a：（色泽=乌黑；根蒂=稍蜷；敲声=沉闷）
2. 数据集：一组样本的集合
3. 属性空间/样本空间：对象属性张成的空间
4. 样例：标记好信息的实例 eg. 西瓜a：（（色泽=乌黑；根蒂=稍蜷；敲声=沉闷），熟瓜）
5. 标记空间：所有标记的集合
6. 概念：我们希望机器从样例中不断归纳学到的东西，大约相当于一个从样本空间到标记空间的函数
7. 假设：从样本空间到标记空间的函数
8. 假设空间：所有可能的假设构成的集合
<!-- slide -->
### 概率统计回顾
1. 总体：是指对准备进行其测量，研究或分析的整个群体。 eg. 世界上所有西瓜
2. 样本：从总体中随机抽取的部分对象。（独立同分布）
3. 大数定律：当试验次数很大时所呈现的概率性质的定律，不是经验规律，而是严格证明了的定理
 
   $\lim_{x \rightarrow \infty}{P\{|\frac{1}{n}\Sigma_{k=1}^nx_k-E(x)|<\epsilon\}}=1$
   
## 新概念

以后主要围绕二分类问题进行讨论，自带上帝视角，也就是说我们对机器要学习的问题知根知底

给定样例集 $D=\{(\boldsymbol{x_1},\mathcal{y}_1),(\boldsymbol{x_2},\mathcal{y}_2),...,(\boldsymbol{x_m},\mathcal{y}_m)\}$，其中$\boldsymbol{x_i}\in \mathcal{X}$，我们讨论的是二分类问题，不妨设$y_i\in \mathcal{Y}=\{-1.+1\}$

样例集$D$中，每一个样例$(\boldsymbol{x_i},\mathcal{y}_i)$里面的样本$\boldsymbol{x_i}$服从一个未知分布$\mathcal{D}$

对于一个$\mathcal{X}$到$\mathcal{Y}$的映射$h$

### 经验误差

$\hat{E}(h;D)=\frac{1}{m}\sum_{i=1}^m\mathbb{I}(h(\boldsymbol{x_i})\neq y_i)$

### 泛化误差

$E(h;\mathcal{D})=P_{x\sim \mathcal{D}}(h(\boldsymbol{x})\neq y)$

### 一致

如果$h$在数据集$D$上经验误差为0，那么就说$h$与$D$一致

### 不合

用于度量两个映射 $h_1,h_2\in \mathcal{Y}^\mathcal{X}$之间的差别

$d(h_1,h_2)=P_{x\sim \mathcal{D}}(h_1(\boldsymbol{x})\neq h_2(\boldsymbol{x}))$

# PAC学习

PAC：Probably Approximately Correct 概率近似正确

设目标概念是$c$,目标概念的集合是$\mathcal{C}$

有机器学习算法$\mathcal{L}$，它能考虑到的所有可能的概念构成集合$\mathcal{H}$，也就是假设空间。

机器学习的任务就是根据训练集$D$,用算法$\mathcal{L}$在假设空间$\mathcal{H}$中找到最接近概念$c$的假设$h$

## 怎样才算学会了——PAC辨识

对$0<\epsilon,\delta<1$，分布$\mathcal{D}$，所有$c\in \mathcal{C}$，若存在学习算法$\mathcal{L}$，其输出的假设$h\in \mathcal{H}$满足

$P(E(h)\leq \epsilon)\geq 1-\delta$

## 能学会吗——PAC可学习

$m$表示从$\mathcal{D}$中独立同分布采样得到的样例数目，也就是样例集$D$的势

如果对于任何

$m\geq poly(1/\epsilon,1\delta,size(\boldsymbol{x}),size(c))$

算法$\mathcal{L}$能实现对概念类$\mathcal{C}$的PAC辨识，那么概念类$\mathcal{C}$就是PAC可学习的

## 学习效率如何——复杂度

### 时间复杂度

如果学习算法$\mathcal{L}$学习概念类$\mathcal{C}$用时为多项式$poly(1/\epsilon,1\delta,size(\boldsymbol{x}),size(c))$，称概念类$\mathcal{C}$是高效PAC可学习的，而算法$\mathcal{L}$是它的PAC学习算法

### 样本复杂度

满足PAC学习算法所需的最小的样例数$m$

# 有限假设空间

现在可以用PAC学习理论来对假设空间$\mathcal{H}$有限时的一些情况进行分析，看看机器学习是否可行

如果$c\in \mathcal{H}$,称这个问题对这个算法是“可分的”、“一致的”

如果$c\notin \mathcal{H}$,称这个问题对这个算法是“不可分的”、“不一致的”

## 问题可分时

目标概念$c\in \mathcal{H}$，最佳情况下算法$\mathcal{L}$能找到一个假设$h\in \mathcal{H}$，使$h=c$，此时$E(h)=0$

但是这一点很难做到，我们只能根据训练集$D$提供的样例从$\mathcal{H}$中排除不符合条件的假设来近似

证明PAC可学习：我们要找到训练集规模$m$，使算法能以概率$1-\delta$找到和目标概念泛化误差不超过$\epsilon$的假设$h$

### 计算过程

取一个泛化误差大于$\epsilon$的假设h

对分布$\mathcal{D}$上取得的任意样例$(\boldsymbol{x},y)$，有

$P(h(x)=y)\\ =1-P(h(x)\neq y)\\=1-E(h)\\<1-\epsilon$

如果数据集$D$给的$m$个样例，$h$的判断都没问题，那算法就有可能把这个不符合要求的假设作为答案

一个不符合$\epsilon$要求的假设$h$被判断为符合要求的可能性

记假设$h$满足数据集$D$，却不满足误差小于$\epsilon$这一事件为$\mathcal{A}(h)$

$P(\mathcal{A}(h))\\=P((h(x_1)=y_1)\wedge...\wedge (h(x_m)=y_m))\\=(1-P(h(\boldsymbol{x})\neq y))^m\\<(1-\epsilon)^m$

$\mathcal{H}$中每一个假设都有可能满足$\mathcal{A}$，则$\mathcal{H}$中存在假设满足$\mathcal{A}$的概率为

$P(\mathcal{A}(h_1)\vee...\vee\mathcal{A}(h_{|\mathcal{H}|}))\\\leq|\mathcal{H}|P(\mathcal{A}(h))\\\leq |\mathcal{H}|(1-\epsilon)^m\\<|\mathcal{H}|e^{-m\epsilon}$

要让这种情况不出现的概率大于$1-\delta$，这种情况出现的概率必须小于$\delta$

则有 $|\mathcal{H}|e^{-m\epsilon}\leq \delta$

可得 $m\geq \frac{1}{\epsilon}(\ln{|
\mathcal{H}|}+ln{\frac{1}{\delta}})$

此时$P(E(h)\leq \epsilon)\geq 1-\delta$

则假设空间有限的可分问题是PAC可学习的，泛化误差随着样例增多收敛到0，收敛速率为$O(\frac{1}{m})$

## 问题不可分时

目标概念$c$不在假设空间$\mathcal{H}$中，任意一个假设或多或少都会有错误

### 不可知PAC可学习

$m$表示训练集大小，如果对任何 $m\geq poly(1/\epsilon,1/\delta,size(\boldsymbol{x}),size(c))$，算法$\mathcal{L}$都能得到假设$h$满足

$P(E(h)-\min_{h'\in \mathcal{H}}E(h'))\geq 1-\sigma$

那么假设空间$\mathcal{H}$就是不可知PAC可学习的

## 经验误差和泛化误差的关系

运用大数定律，经过很多计算之后得到

$P(|E(h)-\hat E(h)|\leq \sqrt{\frac{\ln{|\mathcal{H}|}+\ln{2/\delta}}{2m}})\geq 1-\delta$

也就是说，样本数量大到一定程度后就可以将经验误差看作泛化误差。

我们总能在多项式时间内从有限的假设空间中找到经验误差最小的假设$h$，也就是说假设空间有限的不可分问题也是PAC可学习的

# VC维

假设空间通常是无限的，此时想要用PAC学习理论需要借助VC维的概念，更加准确地描述假设空间的复杂度

## 最后一步放缩

回顾假设空间有限、问题可分时的计算过程，我们在最后一步进行了放缩

(记假设$h$满足数据集$D$，却不满足误差小于$\epsilon$这一事件为$\mathcal{A}(h)$)

$P(\mathcal{A}(h_1)\vee...\vee\mathcal{A}(h_{|\mathcal{H}|}))\\\leq|\mathcal{H}|P(\mathcal{A}(h))$

这一步放缩产生了$|\mathcal{H}|$

实际上，如果假设$h_i$和$h_j$对数据集$D$中的每一个样例判断都一致，则$\mathcal{A}(h_i)$和$\mathcal{A}(h_j)$指的是同一件事情

$P(\mathcal{A}(h_i)\vee \mathcal{A}(h_j))=P(\mathcal{A}(h_i))$

如果$h_{i_1},h_{i_2},...,h_{i_n}$属于同一个等价类$[h_{i}]$

$P(\mathcal{A}(h_{i_1})\vee...\vee \mathcal{A}(h_{i_n}))=P(\mathcal{A}(h_{i}))$

而这样的等价类的数量是有限的，算出等价类数量的上界，就可以避免$|\mathcal{H}|$

## 增长函数

有假设空间$\mathcal{H}$和示例集$D=\{(\boldsymbol{x_1},\mathcal{y}_1),(\boldsymbol{x_2},\mathcal{y}_2),...,(\boldsymbol{x_m},\mathcal{y}_m)\}$

假设空间中的任意一个假设$h$都能对示例集进行标记，得到 $h|_d=\{(h(\boldsymbol{x}_1),h(\boldsymbol{x}_2),...,h(\boldsymbol{x}_m))\}$

定义$\mathcal{H}$中所有假设对$D$能赋予的标记结果的数目为增长函数$\Pi_{\mathcal{H}}(m)$

$\Pi_{\mathcal{H}}(m)=\max_{\{\boldsymbol{x_1,...,x_m}\subseteq\mathcal{X}\}}|\{(h(\boldsymbol{x}_1),h(\boldsymbol{x}_2),...,h(\boldsymbol{x}_m))|h\in\mathcal{H}\}|$

用增长函数代替最后一步放缩，可得

$P\left(|E(h)-\hat E(h)|>\epsilon\right)\leq 4\Pi_\mathcal{H}(2m)e^{-\frac{m\epsilon^2}{8}}$

## 打散

以二分类问题为例

对$m$个示例，最多有$2^m$种标记结果，如果假设空间$\mathcal{H}$能给出$D$的所有标记结果，也就是 $\Pi_{\mathcal{H}}(m)=2^m$ 就说示例集 $D$ 能被假设空间$\mathcal{H}$打散

## VC维——能被打散的最大示例集大小

$VC(\mathcal{H})=max\{m:\Pi_{\mathcal{H}}(m)=2^m\}$

VC维和数据分布$\mathcal{D}$或者具体的数据集$D$无关，它研究的主要是假设空间 $\mathcal{H}$ 的表示能力

### 例：二维平面上的线性划分问题

![例1](http://101.43.171.36/wp-content/uploads/2022/01/VC维例1.png)

## VC维能拿来干什么——更紧的放缩

设假设空间$\mathcal{H}$的VC维为$d$

$\Pi_{\mathcal{H}}(m)\leq {\sum_{i=0}^d}{m \choose i}\leq (\frac{e\cdot m}{d})^d$

可以得到泛化误差界依概率收敛

$P\left(|E(h)-\hat E(h)|\leq\sqrt{\frac{8d\ln{\frac{2em}{d}}+8ln{\frac{4}{\delta}}}{m}}\right)\geq 1-\delta$

收敛速率为$O\left(\frac{1}{\sqrt{m}}\right)$，计算时没有考虑具体的分布$\mathcal{D}$或具体的样例集$D$

接下来，只要算法能让$\hat E(h)=\min_{h'\in \mathcal{H}}\hat E(h')$，满足了**经验风险最小化**即可

借助VC维，我们可以证明任何VC维有限的假设空间$\mathcal{H}$都是PAC可学习的

# Rademacher（拉德马赫尔）复杂度

VC维虽然考虑了假设空间$\mathcal{H}$的结构，但是没有考虑数据分布$\mathcal{D}$，而Rademacher复杂度提供了另一条刻画假设空间复杂度的途径

## 经验Rademacher复杂度

拉德马赫尔随机变量$\sigma$: 0.5概率取1，0.5概率取-1

函数空间$\mathcal{F}:\mathcal{Z}\rightarrow\mathbb{R}$，令$Z\subseteq\mathcal{Z}$

函数空间$\mathcal{F}$关于$Z$的经验Rademacher复杂度

$\hat\mathfrak{R}_Z(\mathcal{F})=\mathbb E_\sigma\left[\sup_{f\in\mathcal{F}}\frac{1}{m}\sum_{i=1}^m\sigma_if(z_i)\right]$

学校$\mathcal{F}$里有许多学生$f$，$Z$是学校考的学科，总共有$m$门。每年学校都会改革，改革之后每个学科占的学分会变，有的学科变得重要了（$\sigma_i=1$），有的学科却没那么重要了（$\sigma_i=-1$），但是每年都会有一个全校第一，全校第一得分的期望值就是经验Rademacher复杂度，它描述了学校里学生的多样性

## Rademacher复杂度

函数空间$\mathcal{F}$关于$\mathcal Z$的Rademacher复杂度

$\mathfrak R_m(\mathcal{F})=\mathbb{E}_{Z\subseteq\mathcal{Z}:|Z|=m}\left[\hat\mathfrak{R}_Z(\mathcal{F})\right]$

所有专业的所有课程为$\mathcal{Z}$，许多所不同的学校都来考察$\mathcal{F}$这帮学生，得到经验Rademacher复杂度的期望，就是Rademacher复杂度。

## Rademacher复杂度的应用

和VC维一样，Rademacher复杂度能推导出泛化误差界

$P\left(|E(h)-\hat E(h)|\leq \mathfrak{R}_m(\mathcal{H})+\sqrt{\frac{\ln{(1/\sigma)}}{2m}} \right)\geq 1-\delta$

而实际上Rademacher复杂度与增长函数是有关系的

$\mathfrak{R}_m(\mathcal{H})\leq\sqrt{\frac{2\ln{\Pi_{\mathcal{H}}(m)}}{m}}$

可以证明一般情况下Rademacher复杂度推导结果比VC维更紧一些

# 稳定性

VC维强调了假设空间$\mathcal{H}$，拉德马赫尔复杂度进一步关注了样本的分布$\mathcal{D}$，而稳定性更加关注具体的算法。

## 损失函数

损失函数$\ell(\mathcal{L}_D(\boldsymbol{x}),y):\mathcal{Y\times Y}\leftarrow \mathbb{R}^+$刻画了假设$\mathcal{L}_D$的预测标记与真实标记之间的差距。

* 泛化损失
  
  $\ell(\mathcal{L},\mathcal{D})=\mathbb{E}_{\boldsymbol{x}\in \mathcal{X},z=(\boldsymbol{x},y)}[\ell(\mathcal{L}_D,z)]$
* 经验损失
  
  $\hat\ell(\mathcal{L},\mathcal{D})=\frac{1}{m}\sum_{i=1}^m\ell(\mathcal{L}_D,z_i)$

* 留一损失

  $\hat\ell(\mathcal{L},\mathcal{D})=\frac{1}{m}\sum_{i=1}^m\ell(\mathcal{L}_{D^{\backslash i}},z_i)$

## 均匀稳定性

对任何$\boldsymbol{x}\in \mathcal{X},z=(\boldsymbol{x},y)$，如果学习算法$\mathcal{L}$满足

$|\ell(\mathcal{L}_D,z)-\ell(\mathcal{L}_{D^{\backslash i}},z)|\leq \beta , i=1,2,...,m$

就说$\mathcal{L}$关于损失函数$\ell$满足$\beta$-均匀稳定性

## 应用

如果$\ell$有界，并且算法$\mathcal{L}$有均匀稳定性，那么经验损失$\hat\ell(\mathcal{L},\mathcal{D})$就会随着m增大依概率收敛到泛化损失$\ell(\mathcal{L},\mathcal{D})$

如果算法$\mathcal{L}$满足经验风险最小化，而且有均匀稳定性，那么假设空间$\mathcal{H}$也是PAC可学习的。

# 参考文献

<https://zhuanlan.zhihu.com/p/337298338>

<https://www.bilibili.com/video/BV1Nr4y1i7or>

周志华. 机器学习[M]. 第一版. 清华大学出版社, 2016 :267-291.